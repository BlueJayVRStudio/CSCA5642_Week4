{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "531a562b-53db-40f4-9f86-ea9c3322b02c",
   "metadata": {},
   "source": [
    "# Week 4 Mini Project: RNN Disaster Tweets Classification\n",
    "\n",
    "By: Jaeyoung Oh\n",
    "\n",
    "Repo: https://github.com/BlueJayVRStudio/CSCA5642_Week4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f81b4f0-7ddd-4e57-b781-317960e08ec3",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "The objective of this week's mini project is to classify the context of Tweets. It is a simple binary classification between whether it is about real disasters or not. The set of data consists of a training set and a test set. The test set is reserved only for submission and not validation. The training set consists of 7613 hand-classified data points each composed of ID, keyword, location, body of text and target label. We will first explore keyword, location and text body to identify potential input columns and then perform necessary preprocessing steps on the selected input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3517d23e-bc9e-455e-a8da-b694960a3483",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1283bb00-d948-4f9c-9810-72f602ab8086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f8da857-84d7-4ea4-a817-033a56c17635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c9658cd-bcc9-4acd-a8e6-93c9054b9288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3263 entries, 0 to 3262\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        3263 non-null   int64 \n",
      " 1   keyword   3237 non-null   object\n",
      " 2   location  2158 non-null   object\n",
      " 3   text      3263 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 102.1+ KB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "859a7281-8989-4e0c-9d65-c136dfaedc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b61dc786-d4d5-4e12-94ba-89651f9209e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4289</th>\n",
       "      <td>6093</td>\n",
       "      <td>hellfire</td>\n",
       "      <td>United Hoods of the Globe</td>\n",
       "      <td>HELLFIRE EP - SILENTMIND &amp;amp; @_bookofdaniel ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5874</th>\n",
       "      <td>8391</td>\n",
       "      <td>ruin</td>\n",
       "      <td>MÌ©rida, YucatÌÁn</td>\n",
       "      <td>babe I'm gonna ruin you if you let me stay</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>538</td>\n",
       "      <td>army</td>\n",
       "      <td>Burbank,CA</td>\n",
       "      <td>@AP what a violent country get the army involv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4343</th>\n",
       "      <td>6167</td>\n",
       "      <td>hijack</td>\n",
       "      <td>Near Richmond, VA</td>\n",
       "      <td>Another Mac vuln!\\n\\nhttps://t.co/OxXRnaB8Un</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6807</th>\n",
       "      <td>9753</td>\n",
       "      <td>tragedy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rly tragedy in MP: Some live to recount horror...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>348</td>\n",
       "      <td>annihilation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Evildead - Annihilation of Civilization http:/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>2881</td>\n",
       "      <td>damage</td>\n",
       "      <td>??? ?? ???????</td>\n",
       "      <td>If Trillion crosses the line a 3rd time he doe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6885</th>\n",
       "      <td>9870</td>\n",
       "      <td>traumatised</td>\n",
       "      <td>ELVY</td>\n",
       "      <td>Think I'm traumatised for life</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6157</th>\n",
       "      <td>8783</td>\n",
       "      <td>siren</td>\n",
       "      <td>Honolulu,Hawaii</td>\n",
       "      <td>Serephina the Siren &amp;lt;3 http://t.co/k6UEtsnLHT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4725</th>\n",
       "      <td>6721</td>\n",
       "      <td>lava</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I lava you ?????? http://t.co/aeZ3aK1lRN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       keyword                   location  \\\n",
       "4289  6093      hellfire  United Hoods of the Globe   \n",
       "5874  8391          ruin          MÌ©rida, YucatÌÁn   \n",
       "375    538          army                 Burbank,CA   \n",
       "4343  6167        hijack          Near Richmond, VA   \n",
       "6807  9753       tragedy                        NaN   \n",
       "245    348  annihilation                        NaN   \n",
       "2005  2881        damage            ??? ?? ???????    \n",
       "6885  9870   traumatised                       ELVY   \n",
       "6157  8783         siren           Honolulu,Hawaii    \n",
       "4725  6721          lava                        NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "4289  HELLFIRE EP - SILENTMIND &amp; @_bookofdaniel ...       0  \n",
       "5874         babe I'm gonna ruin you if you let me stay       0  \n",
       "375   @AP what a violent country get the army involv...       1  \n",
       "4343       Another Mac vuln!\\n\\nhttps://t.co/OxXRnaB8Un       0  \n",
       "6807  Rly tragedy in MP: Some live to recount horror...       1  \n",
       "245   Evildead - Annihilation of Civilization http:/...       0  \n",
       "2005  If Trillion crosses the line a 3rd time he doe...       1  \n",
       "6885                     Think I'm traumatised for life       0  \n",
       "6157   Serephina the Siren &lt;3 http://t.co/k6UEtsnLHT       0  \n",
       "4725           I lava you ?????? http://t.co/aeZ3aK1lRN       0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b0aa6b-992a-4a2a-b7dc-f7b270d1104e",
   "metadata": {},
   "source": [
    "ID has no special meaning, so we can easily drop ID from the dataset. Additionally, there are too many null values in keyword and location. Keyword is selected word from the text so it is redudant information. Although location might provide some special context, there are too many null values and does not contain too much meaning. Therefore, we really only need to consider the text bodies as our input column. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cb460b-c0fa-4017-ada8-ca58b24094a6",
   "metadata": {},
   "source": [
    "Now we will do some basic NLP preprocessing. *Here are some considerations*:\n",
    "1. Most NLP tasks favor removing stop words and porter stemming, but for complex context dependent tasks like identifying real disaster in a tweet, preservation of stop words and suffixes may be quintessential especially taking into account the fact that RNN's can discern sequential/temporal patterns.\n",
    "2. Because RNN learns sequential information, we have to use word tokens as opposed to vectors.\n",
    "3. URL's don't provide enough contextual clues and unnecessarily increase complexity. Thus, we can easily decide to remove URL's.\n",
    "4. For similar reason to removing URL's we can convert all texts to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b97bf2e-134d-4c15-ae0c-978890dfe6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a55e43da-a059-47a8-9a11-179998c8bb42",
   "metadata": {},
   "source": [
    "*Next we look at class distribution to make sure there isn't too much class imbalance*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278879ab-b890-4bbc-a36f-019d098d750f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "753d0ddb-5cfb-43cb-81aa-a31976083216",
   "metadata": {},
   "source": [
    "*Finally, we will do a 80-20 test-validation split on the train dataset*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35a0691d-bf6d-4c98-97f4-968490b5c263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 80-20 test-validation split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_df['text'], train_df['target'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34aed23-e0ca-47b0-8b99-59fe1d00cec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
